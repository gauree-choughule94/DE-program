version: '3.3'
services:
  mysql:
    image: mysql:8.0
    # environment:
    #   MYSQL_ROOT_PASSWORD: root
    #   MYSQL_DATABASE: airflow
    #   MYSQL_USER: airflow
    #   MYSQL_PASSWORD: airflow
    environment:
      - MYSQL_ROOT_PASSWORD=airflow_pass
      # - MYSQL_USER=root
      - MYSQL_PASSWORD=airflow_pass   # add only for non root user
      - MYSQL_DATABASE=airflow
    ports:
      - "3308:3306"
    volumes:
      - mysql_data:/var/lib/mysql
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 10s
      retries: 10

  spark:
    image: bitnami/spark:latest
    container_name: spark_1
    depends_on:
      - mysql
    environment:
      - SPARK_MODE=master
    volumes:
      - ./spark_app:/app
    command: bash -c "sleep infinity"

  nginx:
    image: nginx:latest  # using manually pulled image
    container_name: nginx_container
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro 

  airflow-webserver:
    build: .
    restart: always
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      # AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+mysqldb://root:airflow_pass@mysql/airflow
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+mysqldb://root:airflow_pass@mysql:3306/airflow
      AIRFLOW__CORE__FERNET_KEY: ftN4X-ZmnHGqtVj4qZOgH_DToG5m6flLyGKJQRQ-Ecw=
      AIRFLOW__WEBSERVER__SECRET_KEY: ywVKnN4nI7o8qcJVcOP8FhuBx1ixfKDmX_82L9Drq-Q
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'false'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
    volumes:
      - ./dags:/opt/airflow/dags
    ports:
      - "8090:8080"
    depends_on:
      - airflow-init
      - airflow-scheduler
      - mysql
      - spark
    command: webserver

  airflow-scheduler:
    build: .
    restart: always
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+mysqldb://root:airflow_pass@mysql:3306/airflow
      AIRFLOW__CORE__FERNET_KEY: ftN4X-ZmnHGqtVj4qZOgH_DToG5m6flLyGKJQRQ-Ecw=
      AIRFLOW__WEBSERVER__SECRET_KEY: ywVKnN4nI7o8qcJVcOP8FhuBx1ixfKDmX_82L9Drq-Q
    volumes:
      - ./dags:/opt/airflow/dags
    depends_on:
      - airflow-init
      - mysql
    command: scheduler

  airflow-init:
    build: .
    restart: "no"
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+mysqldb://root:airflow_pass@mysql:3306/airflow
      AIRFLOW__CORE__FERNET_KEY: ftN4X-ZmnHGqtVj4qZOgH_DToG5m6flLyGKJQRQ-Ecw=
      AIRFLOW__WEBSERVER__SECRET_KEY: ywVKnN4nI7o8qcJVcOP8FhuBx1ixfKDmX_82L9Drq-Q
    volumes:
      - ./dags:/opt/airflow/dags
    command: >
      bash -c "
        sleep 10 &&
        airflow db init &&
        airflow users create \
          --username admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com \
          --password admin
      "

volumes:
  mysql_data:

# version: '3.3'
# services:
#   postgres:
#     image: postgres:14
#     environment:
#       POSTGRES_USER: airflow
#       POSTGRES_PASSWORD: airflow_pass
#       POSTGRES_DB: airflow
#     ports:
#       - "5432:5432"

#   spark:
#     image: bitnami/spark:latest
#     container_name: spark_1
#     depends_on:
#       - postgres
#     environment:
#       - SPARK_MODE=master
#     volumes:
#       - ./spark_app:/app
#     command: bash -c "sleep infinity"

#   airflow-webserver:
#     build: .
#     restart: always
#     environment:
#       AIRFLOW__CORE__EXECUTOR: LocalExecutor
#       AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow_pass@postgres:5432/airflow
#       AIRFLOW__CORE__FERNET_KEY: ftN4X-ZmnHGqtVj4qZOgH_DToG5m6flLyGKJQRQ-Ecw=
#       AIRFLOW__WEBSERVER__SECRET_KEY: ywVKnN4nI7o8qcJVcOP8FhuBx1ixfKDmX_82L9Drq-Q
#       AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'false'
#       AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
#       AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
#     volumes:
#       - ./dags:/opt/airflow/dags
#     ports:
#       - "8090:8080"
#     depends_on:
#       - airflow-init
#       - airflow-scheduler
#       - postgres
#       - spark
#     command: webserver

#   airflow-scheduler:
#     build: .
#     restart: always
#     environment:
#       AIRFLOW__CORE__EXECUTOR: LocalExecutor
#       AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow_pass@postgres:5432/airflow
#       AIRFLOW__CORE__FERNET_KEY: ftN4X-ZmnHGqtVj4qZOgH_DToG5m6flLyGKJQRQ-Ecw=
#       AIRFLOW__WEBSERVER__SECRET_KEY: ywVKnN4nI7o8qcJVcOP8FhuBx1ixfKDmX_82L9Drq-Q
#     volumes:
#       - ./dags:/opt/airflow/dags
#     depends_on:
#       - airflow-init
#       - postgres
#     command: scheduler

#   airflow-init:
#     build: .
#     restart: "no"
#     environment:
#       AIRFLOW__CORE__EXECUTOR: LocalExecutor
#       AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow_pass@postgres:5432/airflow
#       AIRFLOW__CORE__FERNET_KEY: ftN4X-ZmnHGqtVj4qZOgH_DToG5m6flLyGKJQRQ-Ecw=
#       AIRFLOW__WEBSERVER__SECRET_KEY: ywVKnN4nI7o8qcJVcOP8FhuBx1ixfKDmX_82L9Drq-Q
#     volumes:
#       - ./dags:/opt/airflow/dags
#     command: >
#       bash -c "
#         sleep 10 &&
#         airflow db init &&
#         airflow users create \
#           --username admin \
#           --firstname Admin \
#           --lastname User \
#           --role Admin \
#           --email admin@example.com \
#           --password admin
#       "

# volumes:
#   postgres_data:
