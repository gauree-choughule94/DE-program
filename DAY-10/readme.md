# PySpark - Topics Covered

## ðŸ”¹ Selecting Columns

- Selecting single/multiple columns using:
  - `df.select("col1", "col2")`
  - `df.select(col("col1"), col("col2"))`
  - `df.selectExpr("col1", "col2 as alias_col2")`

## ðŸ”¹ Filtering Rows

- Using `filter()` and `where()`:

## ðŸ”¹ Adding Columns

- Using `withColumn()` and `lit()`:

## ðŸ”¹ concat

## ðŸ”¹ Renaming Columns

- Using:
  - `withColumnRenamed("old", "new")`
  - `df.select(col("old").alias("new"))`
  - `selectExpr("old as new")`

---

## ðŸ”¹ Type Casting

## ðŸ”¹ Removing Columns

## ðŸ”¹ Combining DataFrames

- `union()` and `unionAll()` (for DataFrames with same schema)
- `unionByName()` (allows column order to differ)

## ðŸ”¹ Conditional Logic [if-else]

- SQL style: `CASE WHEN` (in Spark SQL queries)
- DataFrame style:
  - `when(condition, value).otherwise(default_value)`

## ðŸ”¹ Unique & Sorted Records

## ðŸ”¹ Counting Rows

## ðŸ”¹ Aggregations

- Using aggregate functions:
  - `count()`, `sum()`, `avg()`, `min()`, `max()`

## ðŸ”¹ Group By

## started working on joins