{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3956a32-21a8-4a5c-9cb5-af0b25635b9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_data = [(1,'manish','patna',\"30-05-2022\"),\n",
    "(2,'vikash','kolkata',\"12-03-2023\"),\n",
    "(3,'nikita','delhi',\"25-06-2023\"),\n",
    "(4,'rahul','ranchi',\"24-03-2023\"),\n",
    "(5,'mahesh','jaipur',\"22-03-2023\"),\n",
    "(6,'prantosh','kolkata',\"18-10-2022\"),\n",
    "(7,'raman','patna',\"30-12-2022\"),\n",
    "(8,'prakash','ranchi',\"24-02-2023\"),\n",
    "(9,'ragini','kolkata',\"03-03-2023\"),\n",
    "(10,'raushan','jaipur',\"05-02-2023\")]\n",
    "\n",
    "customer_schema=['customer_id','customer_name','address','date_of_joining']\n",
    "\n",
    "\n",
    "sales_data = [(1,22,10,\"01-06-2022\"),\n",
    "(1,27,5,\"03-02-2023\"),\n",
    "(2,5,3,\"01-06-2023\"),\n",
    "(5,22,1,\"22-03-2023\"),\n",
    "(7,22,4,\"03-02-2023\"),\n",
    "(9,5,6,\"03-03-2023\"),\n",
    "(2,1,12,\"15-06-2023\"),\n",
    "(1,56,2,\"25-06-2023\"),\n",
    "(5,12,5,\"15-04-2023\"),\n",
    "(11,12,76,\"12-03-2023\")]\n",
    "\n",
    "sales_schema=['customer_id','product_id','quantity','date_of_purchase']\n",
    "\n",
    "\n",
    "product_data = [(1, 'fanta',20),\n",
    "(2, 'dew',22),\n",
    "(5, 'sprite',40),\n",
    "(7, 'redbull',100),\n",
    "(12,'mazza',45),\n",
    "(22,'coke',27),\n",
    "(25,'limca',21),\n",
    "(27,'pepsi',14),\n",
    "(56,'sting',10)]\n",
    "\n",
    "product_schema=['id','name','price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "920107b2-3ce2-413c-88b4-5e9dd5469047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cust_df = spark.createDataFrame(data=customer_data, schema=customer_schema)\n",
    "\n",
    "sales_df = spark.createDataFrame(data=sales_data, schema=sales_schema)\n",
    "\n",
    "product_df = spark.createDataFrame(data=product_data, schema=product_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78d6e450-07a1-4d34-a6e1-8b68307f2416",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+\n|customer_id|customer_name|address|date_of_joining|\n+-----------+-------------+-------+---------------+\n|          1|       manish|  patna|     30-05-2022|\n|          2|       vikash|kolkata|     12-03-2023|\n|          3|       nikita|  delhi|     25-06-2023|\n|          4|        rahul| ranchi|     24-03-2023|\n|          5|       mahesh| jaipur|     22-03-2023|\n|          6|     prantosh|kolkata|     18-10-2022|\n|          7|        raman|  patna|     30-12-2022|\n|          8|      prakash| ranchi|     24-02-2023|\n|          9|       ragini|kolkata|     03-03-2023|\n|         10|      raushan| jaipur|     05-02-2023|\n+-----------+-------------+-------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "cust_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc236642-e5c7-4e23-af74-919961868e85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------+----------------+\n|customer_id|product_id|quantity|date_of_purchase|\n+-----------+----------+--------+----------------+\n|          1|        22|      10|      01-06-2022|\n|          1|        27|       5|      03-02-2023|\n|          2|         5|       3|      01-06-2023|\n|          5|        22|       1|      22-03-2023|\n|          7|        22|       4|      03-02-2023|\n|          9|         5|       6|      03-03-2023|\n|          2|         1|      12|      15-06-2023|\n|          1|        56|       2|      25-06-2023|\n|          5|        12|       5|      15-04-2023|\n|         11|        12|      76|      12-03-2023|\n+-----------+----------+--------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d26fb92-1621-4fb0-84a4-433a44dae897",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "cust_df.join(sales_df, cust_df['customer_id']==sales_df['customer_id'], 'inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b505471-58ed-4884-90b2-7dec996e73d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-260234610128201>:3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# this error comes when same column occurs multiple times\u001B[39;00m\n",
       "\u001B[0;32m----> 3\u001B[0m \u001B[43mcust_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43msales_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcust_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcustomer_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43msales_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcustomer_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minner\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcustomer_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n",
       "\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   2980\u001B[0m \n",
       "\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n",
       "\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [AMBIGUOUS_REFERENCE] Reference `customer_id` is ambiguous, could be: [`customer_id`, `customer_id`]."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-260234610128201>:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# this error comes when same column occurs multiple times\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[43mcust_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43msales_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcust_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcustomer_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43msales_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcustomer_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minner\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcustomer_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   2980\u001B[0m \n\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [AMBIGUOUS_REFERENCE] Reference `customer_id` is ambiguous, could be: [`customer_id`, `customer_id`].",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [AMBIGUOUS_REFERENCE] Reference `customer_id` is ambiguous, could be: [`customer_id`, `customer_id`].",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this error comes when same column occurs multiple times\n",
    "\n",
    "cust_df.join(sales_df, cust_df['customer_id']==sales_df['customer_id'], 'inner')\\\n",
    "    .select('customer_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53a1c67b-995d-4ef8-b1f4-0dd73d1d5a20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|customer_id|\n+-----------+\n|          1|\n|          1|\n|          1|\n|          2|\n|          2|\n|          5|\n|          5|\n|          7|\n|          9|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "cust_df.join(sales_df, cust_df['customer_id']==sales_df['customer_id'], 'inner')\\\n",
    "    .select(sales_df['customer_id']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdbfe6f6-19a6-4a5b-a7d4-9595f54be75f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|product_id|\n+----------+\n|         1|\n|         5|\n|         5|\n|        12|\n|        22|\n|        22|\n|        22|\n|        27|\n|        56|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "cust_df.join(sales_df, cust_df['customer_id']==sales_df['customer_id'], 'inner')\\\n",
    "    .select(sales_df['product_id']).sort('product_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b49a55f-f00f-4709-bec9-fba57a998b5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-431437943855903>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m cust_df\u001B[38;5;241m.\u001B[39mjoin(sales_df, (cust_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcustomer_id\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m==\u001B[39msales_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcustomer_id\u001B[39m\u001B[38;5;124m'\u001B[39m]) \u001B[38;5;241m&\u001B[39m (\u001B[43mcust_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mproduct_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m==\u001B[39msales_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mproduct_id\u001B[39m\u001B[38;5;124m'\u001B[39m]), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minner\u001B[39m\u001B[38;5;124m'\u001B[39m)\\\n",
       "\u001B[1;32m      2\u001B[0m     \u001B[38;5;241m.\u001B[39mselect(sales_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mproduct_id\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39msort(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mproduct_id\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2918\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, item)\u001B[0m\n",
       "\u001B[1;32m   2876\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the column as a :class:`Column`.\u001B[39;00m\n",
       "\u001B[1;32m   2877\u001B[0m \n",
       "\u001B[1;32m   2878\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   2915\u001B[0m \u001B[38;5;124;03m+---+----+\u001B[39;00m\n",
       "\u001B[1;32m   2916\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   2917\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(item, \u001B[38;5;28mstr\u001B[39m):\n",
       "\u001B[0;32m-> 2918\u001B[0m     jc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   2919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Column(jc)\n",
       "\u001B[1;32m   2920\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(item, Column):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `product_id` cannot be resolved. Did you mean one of the following? [`customer_id`, `customer_name`, `address`, `date_of_joining`]."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-431437943855903>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m cust_df\u001B[38;5;241m.\u001B[39mjoin(sales_df, (cust_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcustomer_id\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m==\u001B[39msales_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcustomer_id\u001B[39m\u001B[38;5;124m'\u001B[39m]) \u001B[38;5;241m&\u001B[39m (\u001B[43mcust_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mproduct_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m==\u001B[39msales_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mproduct_id\u001B[39m\u001B[38;5;124m'\u001B[39m]), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minner\u001B[39m\u001B[38;5;124m'\u001B[39m)\\\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;241m.\u001B[39mselect(sales_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mproduct_id\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39msort(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mproduct_id\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2918\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, item)\u001B[0m\n\u001B[1;32m   2876\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the column as a :class:`Column`.\u001B[39;00m\n\u001B[1;32m   2877\u001B[0m \n\u001B[1;32m   2878\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2915\u001B[0m \u001B[38;5;124;03m+---+----+\u001B[39;00m\n\u001B[1;32m   2916\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2917\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(item, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m-> 2918\u001B[0m     jc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Column(jc)\n\u001B[1;32m   2920\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(item, Column):\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `product_id` cannot be resolved. Did you mean one of the following? [`customer_id`, `customer_name`, `address`, `date_of_joining`].",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `product_id` cannot be resolved. Did you mean one of the following? [`customer_id`, `customer_name`, `address`, `date_of_joining`].",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cust_df.join(sales_df, (cust_df['customer_id']==sales_df['customer_id']) & (cust_df['product_id']==sales_df['product_id']), 'inner')\\\n",
    "    .select(sales_df['product_id']).sort('product_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8239d520-801e-490a-92d5-059a1758104d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n|          3|       nikita|  delhi|     25-06-2023|       null|      null|    null|            null|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n|          4|        rahul| ranchi|     24-03-2023|       null|      null|    null|            null|\n|          6|     prantosh|kolkata|     18-10-2022|       null|      null|    null|            null|\n|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n|          8|      prakash| ranchi|     24-02-2023|       null|      null|    null|            null|\n|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n|         10|      raushan| jaipur|     05-02-2023|       null|      null|    null|            null|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "cust_df.join(sales_df, cust_df['customer_id']==sales_df['customer_id'], 'left').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc235bed-4bc8-4e5a-ae72-d8b980f9acae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n|          3|       nikita|  delhi|     25-06-2023|       null|      null|    null|            null|\n|          4|        rahul| ranchi|     24-03-2023|       null|      null|    null|            null|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n|          6|     prantosh|kolkata|     18-10-2022|       null|      null|    null|            null|\n|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n|          8|      prakash| ranchi|     24-02-2023|       null|      null|    null|            null|\n|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n|         10|      raushan| jaipur|     05-02-2023|       null|      null|    null|            null|\n|       null|         null|   null|           null|         11|        12|      76|      12-03-2023|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "## full outer join\n",
    "\n",
    "cust_df.join(sales_df, cust_df['customer_id']==sales_df['customer_id'], 'outer').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ba2019c-9b6a-486d-85da-0569a85af32d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|          3|       nikita|  delhi|     25-06-2023|       null|      null|    null|            null|\n|          4|        rahul| ranchi|     24-03-2023|       null|      null|    null|            null|\n|          6|     prantosh|kolkata|     18-10-2022|       null|      null|    null|            null|\n|          8|      prakash| ranchi|     24-02-2023|       null|      null|    null|            null|\n|         10|      raushan| jaipur|     05-02-2023|       null|      null|    null|            null|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "## customers who have not purchased our product\n",
    "\n",
    "cust_df.join(sales_df, cust_df['customer_id']==sales_df['customer_id'], 'left').where(sales_df['product_id'].isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "733c0c5e-9c21-47be-b53b-fdde1cc632a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------+----------------+---+-------+-----+\n|customer_id|product_id|quantity|date_of_purchase| id|   name|price|\n+-----------+----------+--------+----------------+---+-------+-----+\n|          2|         1|      12|      15-06-2023|  1|  fanta|   20|\n|       null|      null|    null|            null|  2|    dew|   22|\n|          9|         5|       6|      03-03-2023|  5| sprite|   40|\n|          2|         5|       3|      01-06-2023|  5| sprite|   40|\n|       null|      null|    null|            null|  7|redbull|  100|\n|         11|        12|      76|      12-03-2023| 12|  mazza|   45|\n|          5|        12|       5|      15-04-2023| 12|  mazza|   45|\n|          7|        22|       4|      03-02-2023| 22|   coke|   27|\n|          5|        22|       1|      22-03-2023| 22|   coke|   27|\n|          1|        22|      10|      01-06-2022| 22|   coke|   27|\n|       null|      null|    null|            null| 25|  limca|   21|\n|          1|        27|       5|      03-02-2023| 27|  pepsi|   14|\n|          1|        56|       2|      25-06-2023| 56|  sting|   10|\n+-----------+----------+--------+----------------+---+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "sales_df.join(product_df, sales_df['product_id']==product_df['id'], 'right').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "653ebd2a-1b63-4785-add2-c552c7f31367",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+-----------+----------+--------+----------------+\n| id|   name|price|customer_id|product_id|quantity|date_of_purchase|\n+---+-------+-----+-----------+----------+--------+----------------+\n|  1|  fanta|   20|          2|         1|      12|      15-06-2023|\n|  2|    dew|   22|       null|      null|    null|            null|\n|  5| sprite|   40|          9|         5|       6|      03-03-2023|\n|  5| sprite|   40|          2|         5|       3|      01-06-2023|\n|  7|redbull|  100|       null|      null|    null|            null|\n| 12|  mazza|   45|         11|        12|      76|      12-03-2023|\n| 12|  mazza|   45|          5|        12|       5|      15-04-2023|\n| 22|   coke|   27|          7|        22|       4|      03-02-2023|\n| 22|   coke|   27|          5|        22|       1|      22-03-2023|\n| 22|   coke|   27|          1|        22|      10|      01-06-2022|\n| 25|  limca|   21|       null|      null|    null|            null|\n| 27|  pepsi|   14|          1|        27|       5|      03-02-2023|\n| 56|  sting|   10|          1|        56|       2|      25-06-2023|\n+---+-------+-----+-----------+----------+--------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "product_df.join(sales_df, product_df['id']==sales_df['product_id'], 'left').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a25cca05-7a83-4ba4-b25b-1ae6c45c47c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+\n|customer_id|customer_name|address|date_of_joining|\n+-----------+-------------+-------+---------------+\n|          1|       manish|  patna|     30-05-2022|\n|          2|       vikash|kolkata|     12-03-2023|\n|          5|       mahesh| jaipur|     22-03-2023|\n|          7|        raman|  patna|     30-12-2022|\n|          9|       ragini|kolkata|     03-03-2023|\n+-----------+-------------+-------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "## left semi join\n",
    "\n",
    "cust_df.join(sales_df, cust_df['customer_id']==sales_df['customer_id'], 'left_semi').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4c09fb1-523f-4a6c-842c-f3eceba6298b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+\n|customer_id|customer_name|address|date_of_joining|\n+-----------+-------------+-------+---------------+\n|          3|       nikita|  delhi|     25-06-2023|\n|          4|        rahul| ranchi|     24-03-2023|\n|          6|     prantosh|kolkata|     18-10-2022|\n|          8|      prakash| ranchi|     24-02-2023|\n|         10|      raushan| jaipur|     05-02-2023|\n+-----------+-------------+-------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "## left semi join\n",
    "\n",
    "cust_df.join(sales_df, cust_df['customer_id']==sales_df['customer_id'], 'left_anti').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c72d53ed-55db-4662-b2a3-5a1d04f7ec7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[24]: 90"
     ]
    }
   ],
   "source": [
    "sales_df.crossJoin(product_df).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2659dbbc-2413-4749-9224-6e172dd0db12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---------+------+\n| id|    name|salary|     dept|gender|\n+---+--------+------+---------+------+\n|  1|  manish| 50000|       IT|     m|\n|  2|  vikash| 60000|    sales|     m|\n|  3| raushan| 70000|marketing|     m|\n|  4|  mukesh| 80000|       IT|     m|\n|  5|   priti| 90000|    sales|     f|\n|  6|  nikita| 45000|marketing|     f|\n|  7|  ragini| 55000|marketing|     f|\n|  8|   rashi|100000|       IT|     f|\n|  9|  aditya| 65000|       IT|     m|\n| 10|   rahul| 50000|marketing|     m|\n| 11|   rakhi| 50000|       IT|     f|\n| 12|akhilesh| 90000|    sales|     m|\n+---+--------+------+---------+------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_data = [(1,'manish',50000,'IT','m'),\n",
    "(2,'vikash',60000,'sales','m'),\n",
    "(3,'raushan',70000,'marketing','m'),\n",
    "(4,'mukesh',80000,'IT','m'),\n",
    "(5,'priti',90000,'sales','f'),\n",
    "(6,'nikita',45000,'marketing','f'),\n",
    "(7,'ragini',55000,'marketing','f'),\n",
    "(8,'rashi',100000,'IT','f'),\n",
    "(9,'aditya',65000,'IT','m'),\n",
    "(10,'rahul',50000,'marketing','m'),\n",
    "(11,'rakhi',50000,'IT','f'),\n",
    "(12,'akhilesh',90000,'sales','m')]\n",
    "\n",
    "emp_schema = ['id', 'name', 'salary', 'dept', 'gender']\n",
    "\n",
    "emp_df = spark.createDataFrame(data=emp_data, schema=emp_schema)\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "325ff021-86f0-4b0a-a45c-0cc35f05c8c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n|     dept|sum_salary|\n+---------+----------+\n|       IT|    345000|\n|marketing|    220000|\n|    sales|    240000|\n+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "emp_df.groupBy('dept').agg(sum('salary').alias('sum_salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e8b38e7-a36b-46cb-8d6c-195e5e7889fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window1 = Window.partitionBy('column').orderBy('column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d790f2ea-56b0-43e3-9678-b7261df8b9c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---------+------+\n| id|    name|salary|     dept|gender|\n+---+--------+------+---------+------+\n|  1|  manish|345000|       IT|     m|\n|  4|  mukesh|345000|       IT|     m|\n|  8|   rashi|345000|       IT|     f|\n|  9|  aditya|345000|       IT|     m|\n| 11|   rakhi|345000|       IT|     f|\n|  3| raushan|220000|marketing|     m|\n|  6|  nikita|220000|marketing|     f|\n|  7|  ragini|220000|marketing|     f|\n| 10|   rahul|220000|marketing|     m|\n|  2|  vikash|240000|    sales|     m|\n|  5|   priti|240000|    sales|     f|\n| 12|akhilesh|240000|    sales|     m|\n+---+--------+------+---------+------+\n\n"
     ]
    }
   ],
   "source": [
    "window1 = Window.partitionBy('dept')\n",
    "\n",
    "emp_df.withColumn('salary', sum('salary').over(window1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9428c8b2-34b1-42bb-999a-b8bca5fb0725",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---------+------+------------+\n| id|    name|salary|     dept|gender|total_salary|\n+---+--------+------+---------+------+------------+\n|  1|  manish| 50000|       IT|     m|      345000|\n|  4|  mukesh| 80000|       IT|     m|      345000|\n|  8|   rashi|100000|       IT|     f|      345000|\n|  9|  aditya| 65000|       IT|     m|      345000|\n| 11|   rakhi| 50000|       IT|     f|      345000|\n|  3| raushan| 70000|marketing|     m|      220000|\n|  6|  nikita| 45000|marketing|     f|      220000|\n|  7|  ragini| 55000|marketing|     f|      220000|\n| 10|   rahul| 50000|marketing|     m|      220000|\n|  2|  vikash| 60000|    sales|     m|      240000|\n|  5|   priti| 90000|    sales|     f|      240000|\n| 12|akhilesh| 90000|    sales|     m|      240000|\n+---+--------+------+---------+------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.withColumn('total_salary', sum(col('salary')).over(window1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c45926c8-d7b9-49a7-8425-375a25377dd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3583412114917440>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43memp_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrow_no\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrow_number\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mover\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwindow1\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:4758\u001B[0m, in \u001B[0;36mDataFrame.withColumn\u001B[0;34m(self, colName, col)\u001B[0m\n",
       "\u001B[1;32m   4753\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(col, Column):\n",
       "\u001B[1;32m   4754\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m   4755\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_COLUMN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   4756\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(col)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n",
       "\u001B[1;32m   4757\u001B[0m     )\n",
       "\u001B[0;32m-> 4758\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jc\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Window function row_number() requires window to be ordered, please add ORDER BY clause. For example SELECT row_number()(value_expr) OVER (PARTITION BY window_partition ORDER BY window_ordering) from table."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-3583412114917440>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43memp_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrow_no\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrow_number\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mover\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwindow1\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:4758\u001B[0m, in \u001B[0;36mDataFrame.withColumn\u001B[0;34m(self, colName, col)\u001B[0m\n\u001B[1;32m   4753\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(col, Column):\n\u001B[1;32m   4754\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m   4755\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_COLUMN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   4756\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(col)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m   4757\u001B[0m     )\n\u001B[0;32m-> 4758\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jc\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Window function row_number() requires window to be ordered, please add ORDER BY clause. For example SELECT row_number()(value_expr) OVER (PARTITION BY window_partition ORDER BY window_ordering) from table.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Window function row_number() requires window to be ordered, please add ORDER BY clause. For example SELECT row_number()(value_expr) OVER (PARTITION BY window_partition ORDER BY window_ordering) from table.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "emp_df.withColumn('row_no', row_number().over(window1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6482eb43-5def-43ef-84f9-e91ca29a683b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---------+------+------+\n| id|    name|salary|     dept|gender|row_no|\n+---+--------+------+---------+------+------+\n|  1|  manish| 50000|       IT|     m|     1|\n| 11|   rakhi| 50000|       IT|     f|     2|\n|  9|  aditya| 65000|       IT|     m|     3|\n|  4|  mukesh| 80000|       IT|     m|     4|\n|  8|   rashi|100000|       IT|     f|     5|\n|  6|  nikita| 45000|marketing|     f|     1|\n| 10|   rahul| 50000|marketing|     m|     2|\n|  7|  ragini| 55000|marketing|     f|     3|\n|  3| raushan| 70000|marketing|     m|     4|\n|  2|  vikash| 60000|    sales|     m|     1|\n|  5|   priti| 90000|    sales|     f|     2|\n| 12|akhilesh| 90000|    sales|     m|     3|\n+---+--------+------+---------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "window1 = Window.partitionBy('dept').orderBy('salary')\n",
    "\n",
    "emp_df.withColumn('row_no', row_number().over(window1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3916c9f-b4cd-4628-955b-cb29c2d3f960",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---------+------+------+\n| id|    name|salary|     dept|gender|row_no|\n+---+--------+------+---------+------+------+\n| 11|   rakhi| 50000|       IT|     f|     1|\n|  8|   rashi|100000|       IT|     f|     2|\n|  1|  manish| 50000|       IT|     m|     1|\n|  9|  aditya| 65000|       IT|     m|     2|\n|  4|  mukesh| 80000|       IT|     m|     3|\n|  6|  nikita| 45000|marketing|     f|     1|\n|  7|  ragini| 55000|marketing|     f|     2|\n| 10|   rahul| 50000|marketing|     m|     1|\n|  3| raushan| 70000|marketing|     m|     2|\n|  5|   priti| 90000|    sales|     f|     1|\n|  2|  vikash| 60000|    sales|     m|     1|\n| 12|akhilesh| 90000|    sales|     m|     2|\n+---+--------+------+---------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "window1 = Window.partitionBy('dept', 'gender').orderBy('salary')\n",
    "\n",
    "emp_df.withColumn('row_no', row_number().over(window1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faff3c59-6635-4799-a0fc-cc5e776404fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---------+------+------+----+----------+\n| id|    name|salary|     dept|gender|row_no|rank|dense_rank|\n+---+--------+------+---------+------+------+----+----------+\n|  1|  manish| 50000|       IT|     m|     1|   1|         1|\n| 11|   rakhi| 50000|       IT|     f|     2|   1|         1|\n|  9|  aditya| 65000|       IT|     m|     3|   3|         2|\n|  4|  mukesh| 80000|       IT|     m|     4|   4|         3|\n|  8|   rashi|100000|       IT|     f|     5|   5|         4|\n|  6|  nikita| 45000|marketing|     f|     1|   1|         1|\n| 10|   rahul| 50000|marketing|     m|     2|   2|         2|\n|  7|  ragini| 55000|marketing|     f|     3|   3|         3|\n|  3| raushan| 70000|marketing|     m|     4|   4|         4|\n|  2|  vikash| 60000|    sales|     m|     1|   1|         1|\n|  5|   priti| 90000|    sales|     f|     2|   2|         2|\n| 12|akhilesh| 90000|    sales|     m|     3|   2|         2|\n+---+--------+------+---------+------+------+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "window1 = Window.partitionBy('dept').orderBy('salary')\n",
    "\n",
    "emp_df.withColumn('row_no', row_number().over(window1))\\\n",
    "    .withColumn('rank', rank().over(window1))\\\n",
    "    .withColumn('dense_rank', dense_rank().over(window1))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2abd040-9455-4b5e-bf90-0746324b7fee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---------+------+------+----+----------+\n| id|    name|salary|     dept|gender|row_no|rank|dense_rank|\n+---+--------+------+---------+------+------+----+----------+\n| 11|   rakhi| 50000|       IT|     f|     1|   1|         1|\n|  8|   rashi|100000|       IT|     f|     2|   2|         2|\n|  1|  manish| 50000|       IT|     m|     1|   1|         1|\n|  9|  aditya| 65000|       IT|     m|     2|   2|         2|\n|  4|  mukesh| 80000|       IT|     m|     3|   3|         3|\n|  6|  nikita| 45000|marketing|     f|     1|   1|         1|\n|  7|  ragini| 55000|marketing|     f|     2|   2|         2|\n| 10|   rahul| 50000|marketing|     m|     1|   1|         1|\n|  3| raushan| 70000|marketing|     m|     2|   2|         2|\n|  5|   priti| 90000|    sales|     f|     1|   1|         1|\n|  2|  vikash| 60000|    sales|     m|     1|   1|         1|\n| 12|akhilesh| 90000|    sales|     m|     2|   2|         2|\n+---+--------+------+---------+------+------+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "window1 = Window.partitionBy('dept', 'gender').orderBy('salary')\n",
    "\n",
    "emp_df.withColumn('row_no', row_number().over(window1))\\\n",
    "    .withColumn('rank', rank().over(window1))\\\n",
    "    .withColumn('dense_rank', dense_rank().over(window1))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f9ebeb3-ac22-498e-8523-f6bbe0b161fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3583412114917444>:7\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# want higher salary employees with dense_rank <= 2\u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m window1 \u001B[38;5;241m=\u001B[39m Window\u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdept\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39morderBy(desc(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
       "\u001B[1;32m      5\u001B[0m emp_df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrow_no\u001B[39m\u001B[38;5;124m'\u001B[39m, row_number()\u001B[38;5;241m.\u001B[39mover(window1))\\\n",
       "\u001B[1;32m      6\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrank\u001B[39m\u001B[38;5;124m'\u001B[39m, rank()\u001B[38;5;241m.\u001B[39mover(window1))\\\n",
       "\u001B[0;32m----> 7\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdense_rank\u001B[39m\u001B[38;5;124m'\u001B[39m, dense_rank()\u001B[38;5;241m.\u001B[39mover(window1))\u001B[38;5;241m.\u001B[39mfilter(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdense_rank\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\\\n",
       "\u001B[1;32m      8\u001B[0m     \u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: '<=' not supported between instances of 'str' and 'int'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-3583412114917444>:7\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# want higher salary employees with dense_rank <= 2\u001B[39;00m\n\u001B[1;32m      3\u001B[0m window1 \u001B[38;5;241m=\u001B[39m Window\u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdept\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39morderBy(desc(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m      5\u001B[0m emp_df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrow_no\u001B[39m\u001B[38;5;124m'\u001B[39m, row_number()\u001B[38;5;241m.\u001B[39mover(window1))\\\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrank\u001B[39m\u001B[38;5;124m'\u001B[39m, rank()\u001B[38;5;241m.\u001B[39mover(window1))\\\n\u001B[0;32m----> 7\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdense_rank\u001B[39m\u001B[38;5;124m'\u001B[39m, dense_rank()\u001B[38;5;241m.\u001B[39mover(window1))\u001B[38;5;241m.\u001B[39mfilter(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdense_rank\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\\\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;241m.\u001B[39mshow()\n\n\u001B[0;31mTypeError\u001B[0m: '<=' not supported between instances of 'str' and 'int'",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: '<=' not supported between instances of 'str' and 'int'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# want higher salary employees with dense_rank <= 2\n",
    "\n",
    "window1 = Window.partitionBy('dept').orderBy(desc('salary'))\n",
    "\n",
    "emp_df.withColumn('row_no', row_number().over(window1))\\\n",
    "    .withColumn('rank', rank().over(window1))\\\n",
    "    .withColumn('dense_rank', dense_rank().over(window1)).filter('dense_rank'<=2)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f2547a5-5c66-4510-84ff-66b5b134897c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---------+------+------+----+----------+\n| id|    name|salary|     dept|gender|row_no|rank|dense_rank|\n+---+--------+------+---------+------+------+----+----------+\n|  8|   rashi|100000|       IT|     f|     1|   1|         1|\n|  4|  mukesh| 80000|       IT|     m|     2|   2|         2|\n|  3| raushan| 70000|marketing|     m|     1|   1|         1|\n|  7|  ragini| 55000|marketing|     f|     2|   2|         2|\n|  5|   priti| 90000|    sales|     f|     1|   1|         1|\n| 12|akhilesh| 90000|    sales|     m|     2|   1|         1|\n|  2|  vikash| 60000|    sales|     m|     3|   3|         2|\n+---+--------+------+---------+------+------+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# want higher salary employees with dense_rank <= 2\n",
    "\n",
    "window1 = Window.partitionBy('dept').orderBy(desc('salary'))\n",
    "\n",
    "emp_df.withColumn('row_no', row_number().over(window1))\\\n",
    "    .withColumn('rank', rank().over(window1))\\\n",
    "    .withColumn('dense_rank', dense_rank().over(window1)).filter(col('dense_rank')<=2)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee23a05b-535a-4718-931a-8dc1176881d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---------+------+------+----+----------+\n| id|    name|salary|     dept|gender|row_no|rank|dense_rank|\n+---+--------+------+---------+------+------+----+----------+\n|  8|   rashi|100000|       IT|     f|     1|   1|         1|\n| 11|   rakhi| 50000|       IT|     f|     2|   2|         2|\n|  4|  mukesh| 80000|       IT|     m|     1|   1|         1|\n|  9|  aditya| 65000|       IT|     m|     2|   2|         2|\n|  7|  ragini| 55000|marketing|     f|     1|   1|         1|\n|  6|  nikita| 45000|marketing|     f|     2|   2|         2|\n|  3| raushan| 70000|marketing|     m|     1|   1|         1|\n| 10|   rahul| 50000|marketing|     m|     2|   2|         2|\n|  5|   priti| 90000|    sales|     f|     1|   1|         1|\n| 12|akhilesh| 90000|    sales|     m|     1|   1|         1|\n|  2|  vikash| 60000|    sales|     m|     2|   2|         2|\n+---+--------+------+---------+------+------+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# want higher salary employees (gendewise) with dense_rank <= 2\n",
    "\n",
    "window1 = Window.partitionBy('dept', 'gender').orderBy(desc('salary'))\n",
    "\n",
    "emp_df.withColumn('row_no', row_number().over(window1))\\\n",
    "    .withColumn('rank', rank().over(window1))\\\n",
    "    .withColumn('dense_rank', dense_rank().over(window1)).filter(col('dense_rank')<=2)\\\n",
    "    .show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2125e780-c26c-46f9-8b7d-c597a461afda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+\n|product_id|product_name|sales_date|  sales|\n+----------+------------+----------+-------+\n|         1|      iphone|01-01-2023|1500000|\n|         2|     samsung|01-01-2023|1100000|\n|         3|     oneplus|01-01-2023|1100000|\n|         1|      iphone|01-02-2023|1300000|\n|         2|     samsung|01-02-2023|1120000|\n|         3|     oneplus|01-02-2023|1120000|\n|         1|      iphone|01-03-2023|1600000|\n|         2|     samsung|01-03-2023|1080000|\n|         3|     oneplus|01-03-2023|1160000|\n|         1|      iphone|01-04-2023|1700000|\n|         2|     samsung|01-04-2023|1800000|\n|         3|     oneplus|01-04-2023|1170000|\n|         1|      iphone|01-05-2023|1200000|\n|         2|     samsung|01-05-2023| 980000|\n|         3|     oneplus|01-05-2023|1175000|\n|         1|      iphone|01-06-2023|1100000|\n|         2|     samsung|01-06-2023|1100000|\n|         3|     oneplus|01-06-2023|1200000|\n+----------+------------+----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "product_data1 = [\n",
    "(1,\"iphone\",\"01-01-2023\",1500000),\n",
    "(2,\"samsung\",\"01-01-2023\",1100000),\n",
    "(3,\"oneplus\",\"01-01-2023\",1100000),\n",
    "(1,\"iphone\",\"01-02-2023\",1300000),\n",
    "(2,\"samsung\",\"01-02-2023\",1120000),\n",
    "(3,\"oneplus\",\"01-02-2023\",1120000),\n",
    "(1,\"iphone\",\"01-03-2023\",1600000),\n",
    "(2,\"samsung\",\"01-03-2023\",1080000),\n",
    "(3,\"oneplus\",\"01-03-2023\",1160000),\n",
    "(1,\"iphone\",\"01-04-2023\",1700000),\n",
    "(2,\"samsung\",\"01-04-2023\",1800000),\n",
    "(3,\"oneplus\",\"01-04-2023\",1170000),\n",
    "(1,\"iphone\",\"01-05-2023\",1200000),\n",
    "(2,\"samsung\",\"01-05-2023\",980000),\n",
    "(3,\"oneplus\",\"01-05-2023\",1175000),\n",
    "(1,\"iphone\",\"01-06-2023\",1100000),\n",
    "(2,\"samsung\",\"01-06-2023\",1100000),\n",
    "(3,\"oneplus\",\"01-06-2023\",1200000)\n",
    "]\n",
    "\n",
    "prod_schema1 = ['product_id', 'product_name', 'sales_date', 'sales']\n",
    "\n",
    "l_l_df = spark.createDataFrame(data=product_data1, schema=prod_schema1)\n",
    "l_l_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e61e632a-9a06-4dd4-b2df-6b01f591e4e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+---------+\n|product_id|product_name|sales_date|  sales|prev_sale|\n+----------+------------+----------+-------+---------+\n|         1|      iphone|01-01-2023|1500000|     null|\n|         1|      iphone|01-02-2023|1300000|  1500000|\n|         1|      iphone|01-03-2023|1600000|  1300000|\n|         1|      iphone|01-04-2023|1700000|  1600000|\n|         1|      iphone|01-05-2023|1200000|  1700000|\n|         1|      iphone|01-06-2023|1100000|  1200000|\n|         2|     samsung|01-01-2023|1100000|     null|\n|         2|     samsung|01-02-2023|1120000|  1100000|\n|         2|     samsung|01-03-2023|1080000|  1120000|\n|         2|     samsung|01-04-2023|1800000|  1080000|\n|         2|     samsung|01-05-2023| 980000|  1800000|\n|         2|     samsung|01-06-2023|1100000|   980000|\n|         3|     oneplus|01-01-2023|1100000|     null|\n|         3|     oneplus|01-02-2023|1120000|  1100000|\n|         3|     oneplus|01-03-2023|1160000|  1120000|\n|         3|     oneplus|01-04-2023|1170000|  1160000|\n|         3|     oneplus|01-05-2023|1175000|  1170000|\n|         3|     oneplus|01-06-2023|1200000|  1175000|\n+----------+------------+----------+-------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "win_ll = Window.partitionBy('product_id').orderBy('sales_date')\n",
    "\n",
    "l_l_df.withColumn('prev_sale', lag(col('sales')).over(win_ll)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a340ac8-eaf7-472b-abce-350cbb62c0e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+---------+\n|product_id|product_name|sales_date|  sales|prev_sale|\n+----------+------------+----------+-------+---------+\n|         1|      iphone|01-01-2023|1500000|     null|\n|         1|      iphone|01-02-2023|1300000|  1500000|\n|         1|      iphone|01-03-2023|1600000|  1300000|\n|         1|      iphone|01-04-2023|1700000|  1600000|\n|         1|      iphone|01-05-2023|1200000|  1700000|\n|         1|      iphone|01-06-2023|1100000|  1200000|\n|         2|     samsung|01-01-2023|1100000|     null|\n|         2|     samsung|01-02-2023|1120000|  1100000|\n|         2|     samsung|01-03-2023|1080000|  1120000|\n|         2|     samsung|01-04-2023|1800000|  1080000|\n|         2|     samsung|01-05-2023| 980000|  1800000|\n|         2|     samsung|01-06-2023|1100000|   980000|\n|         3|     oneplus|01-01-2023|1100000|     null|\n|         3|     oneplus|01-02-2023|1120000|  1100000|\n|         3|     oneplus|01-03-2023|1160000|  1120000|\n|         3|     oneplus|01-04-2023|1170000|  1160000|\n|         3|     oneplus|01-05-2023|1175000|  1170000|\n|         3|     oneplus|01-06-2023|1200000|  1175000|\n+----------+------------+----------+-------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "win_ll = Window.partitionBy('product_id').orderBy('sales_date')\n",
    "\n",
    "l_l_df.withColumn('prev_sale', lag(col('sales'), 1).over(win_ll)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9999bfa5-6703-46c3-ab12-a584bfddcc02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+---------+\n|product_id|product_name|sales_date|  sales|next_sale|\n+----------+------------+----------+-------+---------+\n|         1|      iphone|01-01-2023|1500000|  1300000|\n|         1|      iphone|01-02-2023|1300000|  1600000|\n|         1|      iphone|01-03-2023|1600000|  1700000|\n|         1|      iphone|01-04-2023|1700000|  1200000|\n|         1|      iphone|01-05-2023|1200000|  1100000|\n|         1|      iphone|01-06-2023|1100000|     null|\n|         2|     samsung|01-01-2023|1100000|  1120000|\n|         2|     samsung|01-02-2023|1120000|  1080000|\n|         2|     samsung|01-03-2023|1080000|  1800000|\n|         2|     samsung|01-04-2023|1800000|   980000|\n|         2|     samsung|01-05-2023| 980000|  1100000|\n|         2|     samsung|01-06-2023|1100000|     null|\n|         3|     oneplus|01-01-2023|1100000|  1120000|\n|         3|     oneplus|01-02-2023|1120000|  1160000|\n|         3|     oneplus|01-03-2023|1160000|  1170000|\n|         3|     oneplus|01-04-2023|1170000|  1175000|\n|         3|     oneplus|01-05-2023|1175000|  1200000|\n|         3|     oneplus|01-06-2023|1200000|     null|\n+----------+------------+----------+-------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "win_ll = Window.partitionBy('product_id').orderBy('sales_date')\n",
    "\n",
    "l_l_df.withColumn('next_sale', lead(col('sales')).over(win_ll)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce4999a2-e54a-4dfb-914f-6ee32d40e289",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+---------+\n|product_id|product_name|sales_date|  sales|prev_sale|\n+----------+------------+----------+-------+---------+\n|         1|      iphone|01-01-2023|1500000|     null|\n|         1|      iphone|01-02-2023|1300000|     null|\n|         1|      iphone|01-03-2023|1600000|  1500000|\n|         1|      iphone|01-04-2023|1700000|  1300000|\n|         1|      iphone|01-05-2023|1200000|  1600000|\n|         1|      iphone|01-06-2023|1100000|  1700000|\n|         2|     samsung|01-01-2023|1100000|     null|\n|         2|     samsung|01-02-2023|1120000|     null|\n|         2|     samsung|01-03-2023|1080000|  1100000|\n|         2|     samsung|01-04-2023|1800000|  1120000|\n|         2|     samsung|01-05-2023| 980000|  1080000|\n|         2|     samsung|01-06-2023|1100000|  1800000|\n|         3|     oneplus|01-01-2023|1100000|     null|\n|         3|     oneplus|01-02-2023|1120000|     null|\n|         3|     oneplus|01-03-2023|1160000|  1100000|\n|         3|     oneplus|01-04-2023|1170000|  1120000|\n|         3|     oneplus|01-05-2023|1175000|  1160000|\n|         3|     oneplus|01-06-2023|1200000|  1170000|\n+----------+------------+----------+-------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "win_ll = Window.partitionBy('product_id').orderBy('sales_date')\n",
    "\n",
    "l_l_df.withColumn('prev_sale', lag(col('sales'), 2).over(win_ll)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "debe5bf1-92e8-4076-a1fd-4d1c0172345d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+---------+\n|product_id|product_name|sales_date|  sales|prev_sale|\n+----------+------------+----------+-------+---------+\n|         1|      iphone|01-01-2023|1500000|        0|\n|         1|      iphone|01-02-2023|1300000|        0|\n|         1|      iphone|01-03-2023|1600000|  1500000|\n|         1|      iphone|01-04-2023|1700000|  1300000|\n|         1|      iphone|01-05-2023|1200000|  1600000|\n|         1|      iphone|01-06-2023|1100000|  1700000|\n|         2|     samsung|01-01-2023|1100000|        0|\n|         2|     samsung|01-02-2023|1120000|        0|\n|         2|     samsung|01-03-2023|1080000|  1100000|\n|         2|     samsung|01-04-2023|1800000|  1120000|\n|         2|     samsung|01-05-2023| 980000|  1080000|\n|         2|     samsung|01-06-2023|1100000|  1800000|\n|         3|     oneplus|01-01-2023|1100000|        0|\n|         3|     oneplus|01-02-2023|1120000|        0|\n|         3|     oneplus|01-03-2023|1160000|  1100000|\n|         3|     oneplus|01-04-2023|1170000|  1120000|\n|         3|     oneplus|01-05-2023|1175000|  1160000|\n|         3|     oneplus|01-06-2023|1200000|  1170000|\n+----------+------------+----------+-------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "win_ll = Window.partitionBy('product_id').orderBy('sales_date')\n",
    "\n",
    "l_l_df.withColumn('prev_sale', lag(col('sales'), 2, 0).over(win_ll)).show()   # replace null by 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed4ae850-2a81-4083-9457-1f0b153ba893",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+---------+\n|product_id|product_name|sales_date|  sales|prev_sale|\n+----------+------------+----------+-------+---------+\n|         1|      iphone|01-01-2023|1500000|     null|\n|         1|      iphone|01-02-2023|1300000|  1500000|\n|         1|      iphone|01-03-2023|1600000|  1300000|\n|         1|      iphone|01-04-2023|1700000|  1600000|\n|         1|      iphone|01-05-2023|1200000|  1700000|\n|         1|      iphone|01-06-2023|1100000|  1200000|\n|         2|     samsung|01-01-2023|1100000|     null|\n|         2|     samsung|01-02-2023|1120000|  1100000|\n|         2|     samsung|01-03-2023|1080000|  1120000|\n|         2|     samsung|01-04-2023|1800000|  1080000|\n|         2|     samsung|01-05-2023| 980000|  1800000|\n|         2|     samsung|01-06-2023|1100000|   980000|\n|         3|     oneplus|01-01-2023|1100000|     null|\n|         3|     oneplus|01-02-2023|1120000|  1100000|\n|         3|     oneplus|01-03-2023|1160000|  1120000|\n|         3|     oneplus|01-04-2023|1170000|  1160000|\n|         3|     oneplus|01-05-2023|1175000|  1170000|\n|         3|     oneplus|01-06-2023|1200000|  1175000|\n+----------+------------+----------+-------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "win_ll = Window.partitionBy('product_id').orderBy('sales_date')\n",
    "\n",
    "prev_df = l_l_df.withColumn('prev_sale', lag(col('sales')).over(win_ll))\n",
    "prev_df.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e89b5c61-2e52-4d27-86b0-be2cba96c930",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+---------+-----------------+\n|product_id|product_name|sales_date|  sales|prev_sale|per_loss_and_gain|\n+----------+------------+----------+-------+---------+-----------------+\n|         1|      iphone|01-01-2023|1500000|     null|             null|\n|         1|      iphone|01-02-2023|1300000|  1500000|          -200000|\n|         1|      iphone|01-03-2023|1600000|  1300000|           300000|\n|         1|      iphone|01-04-2023|1700000|  1600000|           100000|\n|         1|      iphone|01-05-2023|1200000|  1700000|          -500000|\n|         1|      iphone|01-06-2023|1100000|  1200000|          -100000|\n|         2|     samsung|01-01-2023|1100000|     null|             null|\n|         2|     samsung|01-02-2023|1120000|  1100000|            20000|\n|         2|     samsung|01-03-2023|1080000|  1120000|           -40000|\n|         2|     samsung|01-04-2023|1800000|  1080000|           720000|\n|         2|     samsung|01-05-2023| 980000|  1800000|          -820000|\n|         2|     samsung|01-06-2023|1100000|   980000|           120000|\n|         3|     oneplus|01-01-2023|1100000|     null|             null|\n|         3|     oneplus|01-02-2023|1120000|  1100000|            20000|\n|         3|     oneplus|01-03-2023|1160000|  1120000|            40000|\n|         3|     oneplus|01-04-2023|1170000|  1160000|            10000|\n|         3|     oneplus|01-05-2023|1175000|  1170000|             5000|\n|         3|     oneplus|01-06-2023|1200000|  1175000|            25000|\n+----------+------------+----------+-------+---------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "prev_df.withColumn('per_loss_and_gain', (col('sales')-col('prev_sale'))).show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e511f5e0-e1e6-4870-8e3a-793695115ff7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+---------+-------------------+\n|product_id|product_name|sales_date|  sales|prev_sale|  per_loss_and_gain|\n+----------+------------+----------+-------+---------+-------------------+\n|         1|      iphone|01-01-2023|1500000|     null|               null|\n|         1|      iphone|01-02-2023|1300000|  1500000|-15.384615384615385|\n|         1|      iphone|01-03-2023|1600000|  1300000|              18.75|\n|         1|      iphone|01-04-2023|1700000|  1600000|   5.88235294117647|\n|         1|      iphone|01-05-2023|1200000|  1700000| -41.66666666666667|\n|         1|      iphone|01-06-2023|1100000|  1200000| -9.090909090909092|\n|         2|     samsung|01-01-2023|1100000|     null|               null|\n|         2|     samsung|01-02-2023|1120000|  1100000| 1.7857142857142856|\n|         2|     samsung|01-03-2023|1080000|  1120000|-3.7037037037037033|\n|         2|     samsung|01-04-2023|1800000|  1080000|               40.0|\n|         2|     samsung|01-05-2023| 980000|  1800000|  -83.6734693877551|\n|         2|     samsung|01-06-2023|1100000|   980000| 10.909090909090908|\n|         3|     oneplus|01-01-2023|1100000|     null|               null|\n|         3|     oneplus|01-02-2023|1120000|  1100000| 1.7857142857142856|\n|         3|     oneplus|01-03-2023|1160000|  1120000| 3.4482758620689653|\n|         3|     oneplus|01-04-2023|1170000|  1160000| 0.8547008547008548|\n|         3|     oneplus|01-05-2023|1175000|  1170000|  0.425531914893617|\n|         3|     oneplus|01-06-2023|1200000|  1175000|  2.083333333333333|\n+----------+------------+----------+-------+---------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# percentage loss/gain based on previous month sales\n",
    "\n",
    "prev_df.withColumn('per_loss_and_gain', (col('sales')-col('prev_sale'))/ col('sales') *100).show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a61e1dbb-bafd-4b3e-8fa3-9c2be3173f8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+---------+-----------------+\n|product_id|product_name|sales_date|  sales|prev_sale|per_loss_and_gain|\n+----------+------------+----------+-------+---------+-----------------+\n|         1|      iphone|01-01-2023|1500000|     null|             null|\n|         1|      iphone|01-02-2023|1300000|  1500000|            -15.0|\n|         1|      iphone|01-03-2023|1600000|  1300000|             19.0|\n|         1|      iphone|01-04-2023|1700000|  1600000|              6.0|\n|         1|      iphone|01-05-2023|1200000|  1700000|            -42.0|\n|         1|      iphone|01-06-2023|1100000|  1200000|             -9.0|\n|         2|     samsung|01-01-2023|1100000|     null|             null|\n|         2|     samsung|01-02-2023|1120000|  1100000|              2.0|\n|         2|     samsung|01-03-2023|1080000|  1120000|             -4.0|\n|         2|     samsung|01-04-2023|1800000|  1080000|             40.0|\n|         2|     samsung|01-05-2023| 980000|  1800000|            -84.0|\n|         2|     samsung|01-06-2023|1100000|   980000|             11.0|\n|         3|     oneplus|01-01-2023|1100000|     null|             null|\n|         3|     oneplus|01-02-2023|1120000|  1100000|              2.0|\n|         3|     oneplus|01-03-2023|1160000|  1120000|              3.0|\n|         3|     oneplus|01-04-2023|1170000|  1160000|              1.0|\n|         3|     oneplus|01-05-2023|1175000|  1170000|              0.0|\n|         3|     oneplus|01-06-2023|1200000|  1175000|              2.0|\n+----------+------------+----------+-------+---------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# percentage loss/gain based on previous month sales\n",
    "\n",
    "prev_df.withColumn('per_loss_and_gain', round((col('sales')-col('prev_sale'))/ col('sales') *100)).show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b92f2fa-2ab4-4ff8-b926-dddb2339975a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-869913061948850>:3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# percentage sales growth\u001B[39;00m\n",
       "\u001B[0;32m----> 3\u001B[0m \u001B[43mprev_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mper_loss_and_gain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mround\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcol\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msales\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43mcol\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mprev_sale\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msales\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: withColumn() takes 3 positional arguments but 4 were given"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-869913061948850>:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# percentage sales growth\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[43mprev_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mper_loss_and_gain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mround\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcol\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msales\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43mcol\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mprev_sale\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msales\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\n\u001B[0;31mTypeError\u001B[0m: withColumn() takes 3 positional arguments but 4 were given",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: withColumn() takes 3 positional arguments but 4 were given",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# percentage loss/gain based on previous month sales\n",
    "\n",
    "prev_df.withColumn('per_loss_and_gain', round((col('sales')-col('prev_sale'))/ col('sales') *100), 2).show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6d83c78-46cc-485b-a826-89ba689b8b17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+---------+-----------------+\n|product_id|product_name|sales_date|  sales|prev_sale|per_loss_and_gain|\n+----------+------------+----------+-------+---------+-----------------+\n|         1|      iphone|01-01-2023|1500000|     null|             null|\n|         1|      iphone|01-02-2023|1300000|  1500000|           -15.38|\n|         1|      iphone|01-03-2023|1600000|  1300000|            18.75|\n|         1|      iphone|01-04-2023|1700000|  1600000|             5.88|\n|         1|      iphone|01-05-2023|1200000|  1700000|           -41.67|\n|         1|      iphone|01-06-2023|1100000|  1200000|            -9.09|\n|         2|     samsung|01-01-2023|1100000|     null|             null|\n|         2|     samsung|01-02-2023|1120000|  1100000|             1.79|\n|         2|     samsung|01-03-2023|1080000|  1120000|             -3.7|\n|         2|     samsung|01-04-2023|1800000|  1080000|             40.0|\n|         2|     samsung|01-05-2023| 980000|  1800000|           -83.67|\n|         2|     samsung|01-06-2023|1100000|   980000|            10.91|\n|         3|     oneplus|01-01-2023|1100000|     null|             null|\n|         3|     oneplus|01-02-2023|1120000|  1100000|             1.79|\n|         3|     oneplus|01-03-2023|1160000|  1120000|             3.45|\n|         3|     oneplus|01-04-2023|1170000|  1160000|             0.85|\n|         3|     oneplus|01-05-2023|1175000|  1170000|             0.43|\n|         3|     oneplus|01-06-2023|1200000|  1175000|             2.08|\n+----------+------------+----------+-------+---------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# percentage loss/gain based on previous month sales\n",
    "\n",
    "prev_df.withColumn('per_loss_and_gain', round((col('sales')-col('prev_sale'))/ col('sales') *100, 2)).show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d646a65-0c36-4809-9e11-7b8283a04bd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+---------+\n|product_id|product_name|sales_date|  sales|prev_sale|\n+----------+------------+----------+-------+---------+\n|         1|      iphone|01-01-2023|1500000|     null|\n|         1|      iphone|01-02-2023|1300000|  1500000|\n|         1|      iphone|01-03-2023|1600000|  1300000|\n|         1|      iphone|01-04-2023|1700000|  1600000|\n|         1|      iphone|01-05-2023|1200000|  1700000|\n|         1|      iphone|01-06-2023|1100000|  1200000|\n|         2|     samsung|01-01-2023|1100000|     null|\n|         2|     samsung|01-02-2023|1120000|  1100000|\n|         2|     samsung|01-03-2023|1080000|  1120000|\n|         2|     samsung|01-04-2023|1800000|  1080000|\n|         2|     samsung|01-05-2023| 980000|  1800000|\n|         2|     samsung|01-06-2023|1100000|   980000|\n|         3|     oneplus|01-01-2023|1100000|     null|\n|         3|     oneplus|01-02-2023|1120000|  1100000|\n|         3|     oneplus|01-03-2023|1160000|  1120000|\n|         3|     oneplus|01-04-2023|1170000|  1160000|\n|         3|     oneplus|01-05-2023|1175000|  1170000|\n|         3|     oneplus|01-06-2023|1200000|  1175000|\n+----------+------------+----------+-------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# percentage of sales each month based on last 6 months\n",
    "\n",
    "prev_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13127162-6e85-4dcf-99b3-bba9087e630c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+----------------+\n|product_id|product_name|sales_date|  sales|each_month_sales|\n+----------+------------+----------+-------+----------------+\n|         1|      iphone|01-01-2023|1500000|         3700000|\n|         2|     samsung|01-01-2023|1100000|         3700000|\n|         3|     oneplus|01-01-2023|1100000|         3700000|\n|         1|      iphone|01-02-2023|1300000|         3540000|\n|         2|     samsung|01-02-2023|1120000|         3540000|\n|         3|     oneplus|01-02-2023|1120000|         3540000|\n|         1|      iphone|01-03-2023|1600000|         3840000|\n|         2|     samsung|01-03-2023|1080000|         3840000|\n|         3|     oneplus|01-03-2023|1160000|         3840000|\n|         1|      iphone|01-04-2023|1700000|         4670000|\n|         2|     samsung|01-04-2023|1800000|         4670000|\n|         3|     oneplus|01-04-2023|1170000|         4670000|\n|         1|      iphone|01-05-2023|1200000|         3355000|\n|         2|     samsung|01-05-2023| 980000|         3355000|\n|         3|     oneplus|01-05-2023|1175000|         3355000|\n|         1|      iphone|01-06-2023|1100000|         3400000|\n|         2|     samsung|01-06-2023|1100000|         3400000|\n|         3|     oneplus|01-06-2023|1200000|         3400000|\n+----------+------------+----------+-------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# percentage of sales each month based on last 6 months\n",
    "win_ll = Window.partitionBy('sales_date')\n",
    "\n",
    "each_month_sales_df = l_l_df.withColumn(\"each_month_sales\", sum(col('sales')).over(win_ll))\n",
    "\n",
    "each_month_sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "342f0a0b-8b3b-4630-a68c-ecf6ffeca840",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+----------------+------------------+\n|product_id|product_name|sales_date|  sales|each_month_sales|            %sales|\n+----------+------------+----------+-------+----------------+------------------+\n|         1|      iphone|01-01-2023|1500000|         3700000|146.66666666666666|\n|         2|     samsung|01-01-2023|1100000|         3700000|236.36363636363637|\n|         3|     oneplus|01-01-2023|1100000|         3700000|236.36363636363637|\n|         1|      iphone|01-02-2023|1300000|         3540000|172.30769230769232|\n|         2|     samsung|01-02-2023|1120000|         3540000|216.07142857142856|\n|         3|     oneplus|01-02-2023|1120000|         3540000|216.07142857142856|\n|         1|      iphone|01-03-2023|1600000|         3840000|             140.0|\n|         2|     samsung|01-03-2023|1080000|         3840000|255.55555555555554|\n|         3|     oneplus|01-03-2023|1160000|         3840000| 231.0344827586207|\n|         1|      iphone|01-04-2023|1700000|         4670000| 174.7058823529412|\n|         2|     samsung|01-04-2023|1800000|         4670000|159.44444444444446|\n|         3|     oneplus|01-04-2023|1170000|         4670000|299.14529914529913|\n|         1|      iphone|01-05-2023|1200000|         3355000|179.58333333333334|\n|         2|     samsung|01-05-2023| 980000|         3355000| 242.3469387755102|\n|         3|     oneplus|01-05-2023|1175000|         3355000|185.53191489361703|\n|         1|      iphone|01-06-2023|1100000|         3400000| 209.0909090909091|\n|         2|     samsung|01-06-2023|1100000|         3400000| 209.0909090909091|\n|         3|     oneplus|01-06-2023|1200000|         3400000|183.33333333333331|\n+----------+------------+----------+-------+----------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# percentage of sales each month based on last 6 months\n",
    "\n",
    "each_month_sales_df.withColumn(\"%sales\", (col('each_month_sales')-col('sales'))/col('sales')*100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6842002e-ad3c-4f85-83d2-a4317f55b153",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "joins_window",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}