# version: '3.3'
# services:
#   mysql:
#     image: mysql:8.0
#     # environment:
#     #   MYSQL_ROOT_PASSWORD: root
#     #   MYSQL_DATABASE: airflow
#     #   MYSQL_USER: airflow
#     #   MYSQL_PASSWORD: airflow
#     environment:
#       - MYSQL_ROOT_PASSWORD=airflow_pass
#       # - MYSQL_USER=root
#       - MYSQL_PASSWORD=airflow_pass   # add only for non root user
#       - MYSQL_DATABASE=airflow
#     ports:
#       - "3310:3306"
#     volumes:
#       - mysql_data:/var/lib/mysql
#       # - ./init.sql:/docker-entrypoint-initdb.d/init.sql
#       - ./mysql:/docker-entrypoint-initdb.d
#     user: "${AIRFLOW_UID}:${AIRFLOW_GID}"

 
#   spark:
#     image: bitnami/spark:latest
#     container_name: spark_incre
#     depends_on:
#       - mysql
#     environment:
#       - SPARK_MODE=master
#     volumes:
#       - ./spark_app:/app
#       - ./data:/opt/airflow/data
#     command: bash -c "sleep infinity"
#     user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
 
#   airflow-webserver:
#     build: .
#     restart: always
#     environment:
#       AIRFLOW__CORE__EXECUTOR: LocalExecutor
#       AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+mysqldb://root:airflow_pass@mysql/airflow
#       AIRFLOW__CORE__FERNET_KEY: SQUIp8dIjPun7E-2CFrtIYGYzUbe9wyJOiPiwiRyKS4=
#       AIRFLOW__WEBSERVER__SECRET_KEY: I3wvFrFOyI1WYhlr9huTw1EQpJpry-sWepa7OdKHbW4
#       AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'false'
#       AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
#       AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
#     volumes:
#       - ./dags:/opt/airflow/dags
#       - ./script:/opt/airflow/script   
#       - ./config:/opt/airflow/config
#       - ./data:/opt/airflow/data
#     ports:
#       - "8099:8080"
#     depends_on:
#       - airflow-init
#       - airflow-scheduler
#     command: webserver
#     user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
 
#   airflow-scheduler:
#     build: .
#     restart: always
#     environment:
#       AIRFLOW__CORE__EXECUTOR: LocalExecutor
#       AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+mysqldb://root:airflow_pass@mysql/airflow
#       AIRFLOW__CORE__FERNET_KEY: SQUIp8dIjPun7E-2CFrtIYGYzUbe9wyJOiPiwiRyKS4=
#       AIRFLOW__WEBSERVER__SECRET_KEY: I3wvFrFOyI1WYhlr9huTw1EQpJpry-sWepa7OdKHbW4
#     volumes:
#       - ./dags:/opt/airflow/dags
#       - ./script:/opt/airflow/script    # ðŸ‘ˆ Add this line
#       - ./config:/opt/airflow/config
#       - ./data:/opt/airflow/data
#     depends_on:
#       - airflow-init
#       - mysql
#     command: scheduler
#     user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
 
#   airflow-init:
#     build: .
#     restart: "no"
#     environment:
#       AIRFLOW__CORE__EXECUTOR: LocalExecutor
#       AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+mysqldb://root:airflow_pass@mysql/airflow
#       AIRFLOW__CORE__FERNET_KEY: SQUIp8dIjPun7E-2CFrtIYGYzUbe9wyJOiPiwiRyKS4=
#       AIRFLOW__WEBSERVER__SECRET_KEY: I3wvFrFOyI1WYhlr9huTw1EQpJpry-sWepa7OdKHbW4
#     volumes:
#       - ./dags:/opt/airflow/dags
#       - ./script:/opt/airflow/script    # ðŸ‘ˆ Add this line
#       - ./config:/opt/airflow/config
#       - ./data:/opt/airflow/data
#     user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
#     command: >
#       bash -c "
#         sleep 10 &&
#         airflow db init &&
#         airflow users create \
#           --username admin \
#           --firstname Admin \
#           --lastname User \
#           --role Admin \
#           --email admin@example.com \
#           --password admin
#       "
 
# volumes:
#   mysql_data:

version: '3.3'

services:
  mysql:
    image: mysql:8.0
    container_name: mysql_1
    environment:
      - MYSQL_ROOT_PASSWORD=airflow_pass
      - MYSQL_DATABASE=airflow
    ports:
      - "3310:3306"
    volumes:
      - mysql_data:/var/lib/mysql
      - ./mysql:/docker-entrypoint-initdb.d
    user: "${AIRFLOW_UID}:${AIRFLOW_GID}"

  spark-master:
    image: bitnami/spark:latest
    container_name: spark_master
    environment:
      - SPARK_MODE=master
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - ./spark_app:/app
      - ./data:/opt/airflow/data
      - ./spark_app/jars:/opt/spark/jars
    user: "${AIRFLOW_UID}:${AIRFLOW_GID}"

  spark-worker:
    image: bitnami/spark:latest
    container_name: spark_worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
    volumes:
      - ./spark_app:/app
      - ./data:/opt/airflow/data
      - ./spark_app/jars:/opt/spark/jars
    user: "${AIRFLOW_UID}:${AIRFLOW_GID}"

  spark-worker-2:
    image: bitnami/spark:latest
    container_name: spark_worker_2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
    volumes:
      - ./spark_app:/app
      - ./data:/opt/airflow/data
      - ./spark_app/jars:/opt/spark/jars
    user: "${AIRFLOW_UID}:${AIRFLOW_GID}"

  jupyter:
    image: jupyter/pyspark-notebook
    container_name: jupyter_spark
    ports:
      - "8888:8888"
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./data:/opt/airflow/data
      - ./spark_app/jars:/opt/spark/jars
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    depends_on:
      - spark-master
      - mysql

  airflow-webserver:
    build: .
    container_name: airflow_webserver
    restart: always
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+mysqldb://root:airflow_pass@mysql/airflow
      AIRFLOW__CORE__FERNET_KEY: SQUIp8dIjPun7E-2CFrtIYGYzUbe9wyJOiPiwiRyKS4=
      AIRFLOW__WEBSERVER__SECRET_KEY: I3wvFrFOyI1WYhlr9huTw1EQpJpry-sWepa7OdKHbW4
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "false"
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./script:/opt/airflow/script
      - ./config:/opt/airflow/config
      - ./data:/opt/airflow/data
    ports:
      - "8099:8080"
    depends_on:
      - airflow-init
      - airflow-scheduler
    command: webserver
    user: "${AIRFLOW_UID}:${AIRFLOW_GID}"

  airflow-scheduler:
    build: .
    container_name: airflow_scheduler
    restart: always
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+mysqldb://root:airflow_pass@mysql/airflow
      AIRFLOW__CORE__FERNET_KEY: SQUIp8dIjPun7E-2CFrtIYGYzUbe9wyJOiPiwiRyKS4=
      AIRFLOW__WEBSERVER__SECRET_KEY: I3wvFrFOyI1WYhlr9huTw1EQpJpry-sWepa7OdKHbW4
    volumes:
      - ./dags:/opt/airflow/dags
      - ./script:/opt/airflow/script
      - ./config:/opt/airflow/config
      - ./data:/opt/airflow/data
    depends_on:
      - airflow-init
      - mysql
    command: scheduler
    user: "${AIRFLOW_UID}:${AIRFLOW_GID}"

  airflow-init:
    build: .
    container_name: airflow_init
    restart: "no"
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+mysqldb://root:airflow_pass@mysql/airflow
      AIRFLOW__CORE__FERNET_KEY: SQUIp8dIjPun7E-2CFrtIYGYzUbe9wyJOiPiwiRyKS4=
      AIRFLOW__WEBSERVER__SECRET_KEY: I3wvFrFOyI1WYhlr9huTw1EQpJpry-sWepa7OdKHbW4
    volumes:
      - ./dags:/opt/airflow/dags
      - ./script:/opt/airflow/script
      - ./config:/opt/airflow/config
      - ./data:/opt/airflow/data
    user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
    command: >
      bash -c "
        sleep 10 &&
        airflow db init &&
        airflow users create \
          --username admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com \
          --password admin
      "

volumes:
  mysql_data: