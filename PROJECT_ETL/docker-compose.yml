version: '3.3'

services:
  mysql_source:
    image: mysql:8.0
    container_name: mysql_source
    restart: unless-stopped
    environment:
      MYSQL_ROOT_PASSWORD: newpassword123
      MYSQL_DATABASE: ecommerce_db
      MYSQL_USER: ecomuser          # used
      MYSQL_PASSWORD: ecompassword  # used
    volumes:
      - mysql_data:/var/lib/mysql
      - ./mysql_init:/docker-entrypoint-initdb.d
    ports:
      - "3308:3306"
    networks:
      - ecommerce_network

  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.3
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - ecommerce_network

  kafka:
    image: confluentinc/cp-kafka:7.5.3
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - ecommerce_network

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    ports:
      - "8005:8005"
    environment:
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:9092
      - SERVER_PORT=8005
    depends_on:
      - kafka
    networks:
      - ecommerce_network

  redis:
    image: redis:7
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - ecommerce_network

  clickhouse:
    image: clickhouse/clickhouse-server:23.3
    container_name: clickhouse
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    ports:
      - "8123:8123"
    volumes:
      - clickhouse_data:/var/lib/clickhouse
    environment:
      - JOIN_ALGORITHM=auto
    healthcheck:
      test: ["CMD-SHELL", "clickhouse-client --query 'SELECT 1' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - ecommerce_network

  postgres_dw:
    image: postgres:15
    container_name: postgres_dw
    restart: unless-stopped
    environment:
      POSTGRES_USER: dw_user
      POSTGRES_PASSWORD: dw_password
      POSTGRES_DB: ecommerce_warehouse
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./postgres_init:/docker-entrypoint-initdb.d
    ports:
      - "5433:5432"
    networks:
      - ecommerce_network

  superset:
    image: apache/superset:latest
    container_name: superset
    depends_on:
      - postgres_dw
    ports:
      - "8088:8088"
    environment:
      SUPERSET_SECRET_KEY: "a_very_strong_secret_key_for_superset"
    command: >
      /bin/bash -c "
      pip install psycopg2-binary &&
      superset db upgrade &&
      superset fab create-admin --username admin --firstname Superset --lastname Admin --email admin@superset.com --password admin &&
      superset init &&
      superset run -h 0.0.0.0 -p 8088"
    volumes:
      - superset_data:/app/superset_home
    networks:
      - ecommerce_network
 
  spark-master:
    image: bitnami/spark:3.5
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - '8080:8080'
      - '7077:7077'
    volumes:
      - ./pyspark_apps:/opt/bitnami/spark/apps
      - ./data_lake:/opt/data_lake
    networks:
      - ecommerce_network
 
  spark-worker:
    image: bitnami/spark:3.5
    container_name: spark-worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ./pyspark_apps:/opt/pyspark_apps
      - ./data_lake:/opt/data_lake
    networks:
      - ecommerce_network
 
  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile
    image: airflow-custom:2.8.0
    container_name: airflow-init
    depends_on:
      - postgres_dw
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://dw_user:dw_password@postgres_dw:5432/ecommerce_warehouse
      - _AIRFLOW_WWW_USER_USERNAME=admin
      - _AIRFLOW_WWW_USER_PASSWORD=admin
    volumes:
      - ./airflow_dags:/opt/airflow/dags
      - ./airflow_logs:/opt/airflow/logs
      - ./airflow_plugins:/opt/airflow/plugins
      - ./pyspark_apps/jars:/opt/jars
      - ./data_lake:/opt/data_lake
    command: >
      bash -c "
        airflow db init &&
        airflow users create --username admin --password admin --firstname Air --lastname Flow --role Admin --email admin@example.com
      "
    networks:
      - ecommerce_network
 
  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile
    image: apache/airflow:2.8.0
    restart: unless-stopped
    depends_on:
      - airflow-init
      - postgres_dw
      - spark-master
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://dw_user:dw_password@postgres_dw:5432/ecommerce_warehouse
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    volumes:
      - ./airflow_dags:/opt/airflow/dags
      - ./airflow_logs:/opt/airflow/logs
      - ./airflow_plugins:/opt/airflow/plugins
      - ./pyspark_apps:/opt/pyspark_apps
      - ./data_lake:/opt/data_lake
      - ./pyspark_apps/jars:/opt/jars
    ports:
      - "8081:8080"
    command: webserver
    networks:
      - ecommerce_network
 
  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile
    image: apache/airflow:2.8.0
    restart: unless-stopped
    depends_on:
      - airflow-init
      - postgres_dw
      - spark-master
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://dw_user:dw_password@postgres_dw:5432/ecommerce_warehouse
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    volumes:
      - ./airflow_dags:/opt/airflow/dags
      - ./airflow_logs:/opt/airflow/logs
      - ./airflow_plugins:/opt/airflow/plugins
      - ./pyspark_apps:/opt/pyspark_apps
      - ./data_lake:/opt/data_lake
      - ./pyspark_apps/jars:/opt/jars
    command: scheduler
    networks:
      - ecommerce_network
 
  # posthog:
  #   image: posthog/posthog:latest
  #   container_name: posthog
  #   depends_on:
  #     - postgres_dw
  #     - clickhouse
  #     - redis
  #   # entrypoint: ["./wait-for-it.sh", "db:5432", "--", "entrypoint.sh"]
  #   ports:
  #     - "8000:8000"
  #   environment:
  #     REDIS_URL: redis://redis:6379/0
  #     CLICKHOUSE_HOST: clickhouse
  #     CLICKHOUSE_SECURE: "false"
  #     CLICKHOUSE_VERIFY: "false"
  #     DATABASE_URL: postgres://dw_user:dw_password@postgres_dw:5432/posthog_db
  #     SECRET_KEY: 2I1LfJiGChWCXtSVuqud6Yw8_V3uBIsvQkzoGoZw6rbYCXQJFq6vUlYh9wfI6S7ImP0
  #     SITE_URL: http://posthog:8000  # üîÅ changed from localhost for cloud   # SITE_URL: https://us.i.posthog.com   # if no docker
  #     CLICKHOUSE_DISABLE_CLUSTER_MIGRATION: "true"
  #     ASYNC_MIGRATIONS: "false"
  #   volumes:
  #     - posthog_data:/var/lib/posthog
  #   restart: unless-stopped
  #   networks:
  #     - ecommerce_network
  kafka_producer:
    build:
      context: ./event_producers  # Assuming Dockerfile is in event_producers
    container_name: kafka_producer
    depends_on:
      - kafka
    volumes:
      - ./event_producers:/app
      - ./data_lake:/opt/data_lake
    networks:
      - ecommerce_network
    # restart: unless-stopped
    restart: on-failure

  event_consumer_posthog:
    build:
      context: ./event_consumers  # Assuming Dockerfile is in event_consumers
    container_name: event_consumer_posthog
    depends_on:
      - kafka
      # - posthog
    environment:
      KAFKA_BOOTSTRAP_SERVERS: "kafka:9092"
      KAFKA_TOPIC: "product_events,order_events"
      KAFKA_CONSUMER_GROUP: "posthog_event_consumer_group"
      POSTHOG_API_KEY: "phc_HqW5rE3RsehAk7gGYV12V4dZKlmcFwAHpdAsJI5flmW"  # Use secrets or .env in production
      # POSTHOG_INSTANCE_URL: "http://posthog:8000"
      POSTHOG_INSTANCE_URL: "https://us.i.posthog.com"
      PYTHONUNBUFFERED: 1  # For real-time logging
    volumes:
      - ./event_consumers:/app
    networks:
      - ecommerce_network
    # restart: unless-stopped
    restart: on-failure
 
volumes:
  mysql_data:
  postgres_data:
  # posthog_data:
  clickhouse_data:
  kafka_data:
  superset_data:
 
networks:
  ecommerce_network:
    driver: bridge

